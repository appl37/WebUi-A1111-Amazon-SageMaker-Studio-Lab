{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Steps to Use the Notebook:\n",
        "## 1.Open the Notebook\n",
        "*   Navigate to the provided Python notebook, titled something like download_models.\n",
        "*   Ensure you're in the correct kernel (py310env for Python 3.10) before running the notebook.\n",
        "## 2.Run the Notebook\n",
        "\n",
        "\n",
        "*   Once the notebook is open, you'll see pre-configured cells for downloading Stable Diffusion models, LoRA, and VAE.\n",
        "*   Simply execute each cell by selecting it and clicking the Run button (or pressing Shift + Enter).\n",
        "\n"
      ],
      "metadata": {
        "id": "ooghwhkoRbW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before Runing Clear .cache"
      ],
      "metadata": {
        "id": "AvzvbS1hGgft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /home/studio-lab-user/.cache\n",
        "!df -h | grep -E 'Avail|home'\n",
        "\n",
        "print('\u001b[1;32mDone!')"
      ],
      "metadata": {
        "id": "BtUwy7GCGn-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models or Checkpoint\n"
      ],
      "metadata": {
        "id": "D3FPHTicN93T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Download path\n",
        "download_path = \"/home/studio-lab-user/stable-diffusion-webui/models/Stable-diffusion/\"\n",
        "\n",
        "# Ensure the download path exists, create it if it doesn't\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Helper function to convert bytes into human-readable format\n",
        "def format_size(size):\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if size < 1024:\n",
        "            return f\"{size:.2f} {unit}\"\n",
        "        size /= 1024\n",
        "\n",
        "# Function to check available storage space\n",
        "def check_storage_space():\n",
        "    result = subprocess.run([\"df\", \"-h\", \"--output=avail,target\"], capture_output=True, text=True)\n",
        "    lines = result.stdout.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        if '/home' in line:\n",
        "            available_space = line.split()[0]  # Get available space value\n",
        "            return available_space\n",
        "    return \"0M\"  # Default value if no storage info available\n",
        "\n",
        "# Function to download model with reduced output frequency\n",
        "def download_model(url, filename, headers):\n",
        "    try:\n",
        "        # Automatically handle the file extension based on the filename provided\n",
        "        response = requests.get(url, headers=headers, stream=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            downloaded_size = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            with open(filename, 'wb') as file:\n",
        "                chunk_size = 1024 * 1024  # Reduce the chunk size\n",
        "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                    if chunk:\n",
        "                        try:\n",
        "                            file.write(chunk)\n",
        "                            downloaded_size += len(chunk)\n",
        "\n",
        "                            if downloaded_size >= 10 * 1024 * 1024:  # Log progress every 10 MB\n",
        "                                elapsed_time = time.time() - start_time\n",
        "                                speed = (downloaded_size / (1024 * 1024)) / elapsed_time\n",
        "                                print(f\"Downloaded {format_size(downloaded_size)} of {format_size(total_size)} at {speed:.2f} MB/s\", end='\\r')\n",
        "                        except OSError as e:\n",
        "                            if \"No space left on device\" in str(e):\n",
        "                                print(f\"\\n\\nError: No space left on device while downloading {filename}.\")\n",
        "                                return \"no_space\"  # Return a specific message if no space\n",
        "                            raise\n",
        "            print(f\"\\n\\nDownloaded {filename} successfully!\\n\")\n",
        "            return \"success\"\n",
        "        else:\n",
        "            print(f\"\\n\\nFailed to download {filename}. Status code: {response.status_code}, Message: {response.text}\\n\")\n",
        "            return \"error\"\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nError during download of {filename}: {e}\\n\")\n",
        "        if \"data exceeds\" in str(e).lower():\n",
        "            print(\"Error: Data exceeds Jupyter memory limit. Please download outside Jupyter Lab.\\n\")\n",
        "        return \"error\"\n",
        "\n",
        "# Combined function to handle both Hugging Face and CivitAI links\n",
        "def download_models(model_list, huggingface_headers, civitai_headers):\n",
        "    for model in model_list:\n",
        "        if model[\"enabled\"]:\n",
        "            url = model[\"link\"]\n",
        "            filename = os.path.join(download_path, model[\"filename\"])\n",
        "\n",
        "            # Identify if it's Hugging Face or CivitAI by checking the URL structure\n",
        "            if \"huggingface.co\" in url:\n",
        "                headers = huggingface_headers\n",
        "                platform = \"Hugging Face\"\n",
        "            elif \"civitai.com\" in url:\n",
        "                headers = civitai_headers\n",
        "                platform = \"CivitAI\"\n",
        "            else:\n",
        "                print(f\"\\n\\nUnknown platform for {filename}. Skipping...\\n\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nStarting download from {platform} for {filename}...\\n\")\n",
        "\n",
        "            # Check storage before downloading\n",
        "            available_space = check_storage_space()\n",
        "            print(f\"Available space before downloading: {available_space}\\n\")\n",
        "\n",
        "            if available_space.endswith('G') or available_space.endswith('M'):\n",
        "                result = download_model(url, filename, headers)\n",
        "                if result == \"no_space\":\n",
        "                    print(f\"\\n\\nNo space available. Stopping remaining downloads.\\n\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"\\n\\nNo space available for {filename}. Skipping...\\n\")\n",
        "        else:\n",
        "            print(f\"\\n\\nSkipping {model['filename']} (Disabled)\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Example combined model inputs (both Hugging Face and CivitAI)\n",
        "model_list = [\n",
        "\n",
        "    {\"link\": \"url_link_1\",\n",
        "     \"filename\": \"model1.safetensors\",     \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_2\",\n",
        "     \"filename\": \"model2.safetensors\",    \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_3\",\n",
        "     \"filename\": \"model3.safetensors\",    \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_4\",\n",
        "     \"filename\": \"model4\",    \"enabled\": False  }\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Token Section (define them here and create headers below)\n",
        "huggingface_token = \"your_huggingface_token\"\n",
        "civitai_api_key = \"your_civitai_token\"\n",
        "\n",
        "\n",
        "\n",
        "# Now, after tokens are defined, create the headers\n",
        "huggingface_headers = {\n",
        "    \"Authorization\": f\"Bearer {huggingface_token}\"\n",
        "}\n",
        "civitai_headers = {\n",
        "    \"Authorization\": f\"Bearer {civitai_api_key}\"\n",
        "}\n",
        "\n",
        "# Call the combined download function\n",
        "download_models(model_list, huggingface_headers, civitai_headers)\n",
        "\n",
        "# Clear cache after downloads\n",
        "print(\"\\n\\nClearing cache...\\n\")\n",
        "subprocess.run([\"rm\", \"-rf\", \".cache\"])\n",
        "\n",
        "# Final storage check\n",
        "print(\"\\nAll downloads done!\")\n",
        "print(f\"Available space after downloading: {check_storage_space()}\\n\")\n"
      ],
      "metadata": {
        "id": "8g-5gJjaOPSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA"
      ],
      "metadata": {
        "id": "E_aeRowA_bXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Download path\n",
        "download_path = \"/home/studio-lab-user/stable-diffusion-webui/models/Lora/\"\n",
        "\n",
        "# Ensure the download path exists, create it if it doesn't\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Helper function to convert bytes into human-readable format\n",
        "def format_size(size):\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if size < 1024:\n",
        "            return f\"{size:.2f} {unit}\"\n",
        "        size /= 1024\n",
        "\n",
        "# Function to check available storage space\n",
        "def check_storage_space():\n",
        "    result = subprocess.run([\"df\", \"-h\", \"--output=avail,target\"], capture_output=True, text=True)\n",
        "    lines = result.stdout.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        if '/home' in line:\n",
        "            available_space = line.split()[0]  # Get available space value\n",
        "            return available_space\n",
        "    return \"0M\"  # Default value if no storage info available\n",
        "\n",
        "# Function to download model with reduced output frequency\n",
        "def download_model(url, filename, headers):\n",
        "    try:\n",
        "        # Automatically handle the file extension based on the filename provided\n",
        "        response = requests.get(url, headers=headers, stream=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            downloaded_size = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            with open(filename, 'wb') as file:\n",
        "                chunk_size = 1024 * 1024  # Reduce the chunk size\n",
        "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                    if chunk:\n",
        "                        try:\n",
        "                            file.write(chunk)\n",
        "                            downloaded_size += len(chunk)\n",
        "\n",
        "                            if downloaded_size >= 10 * 1024 * 1024:  # Log progress every 10 MB\n",
        "                                elapsed_time = time.time() - start_time\n",
        "                                speed = (downloaded_size / (1024 * 1024)) / elapsed_time\n",
        "                                print(f\"Downloaded {format_size(downloaded_size)} of {format_size(total_size)} at {speed:.2f} MB/s\", end='\\r')\n",
        "                        except OSError as e:\n",
        "                            if \"No space left on device\" in str(e):\n",
        "                                print(f\"\\n\\nError: No space left on device while downloading {filename}.\")\n",
        "                                return \"no_space\"  # Return a specific message if no space\n",
        "                            raise\n",
        "            print(f\"\\n\\nDownloaded {filename} successfully!\\n\")\n",
        "            return \"success\"\n",
        "        else:\n",
        "            print(f\"\\n\\nFailed to download {filename}. Status code: {response.status_code}, Message: {response.text}\\n\")\n",
        "            return \"error\"\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nError during download of {filename}: {e}\\n\")\n",
        "        if \"data exceeds\" in str(e).lower():\n",
        "            print(\"Error: Data exceeds Jupyter memory limit. Please download outside Jupyter Lab.\\n\")\n",
        "        return \"error\"\n",
        "\n",
        "# Combined function to handle both Hugging Face and CivitAI links\n",
        "def download_models(model_list, huggingface_headers, civitai_headers):\n",
        "    for model in model_list:\n",
        "        if model[\"enabled\"]:\n",
        "            url = model[\"link\"]\n",
        "            filename = os.path.join(download_path, model[\"filename\"])\n",
        "\n",
        "            # Identify if it's Hugging Face or CivitAI by checking the URL structure\n",
        "            if \"huggingface.co\" in url:\n",
        "                headers = huggingface_headers\n",
        "                platform = \"Hugging Face\"\n",
        "            elif \"civitai.com\" in url:\n",
        "                headers = civitai_headers\n",
        "                platform = \"CivitAI\"\n",
        "            else:\n",
        "                print(f\"\\n\\nUnknown platform for {filename}. Skipping...\\n\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nStarting download from {platform} for {filename}...\\n\")\n",
        "\n",
        "            # Check storage before downloading\n",
        "            available_space = check_storage_space()\n",
        "            print(f\"Available space before downloading: {available_space}\\n\")\n",
        "\n",
        "            if available_space.endswith('G') or available_space.endswith('M'):\n",
        "                result = download_model(url, filename, headers)\n",
        "                if result == \"no_space\":\n",
        "                    print(f\"\\n\\nNo space available. Stopping remaining downloads.\\n\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"\\n\\nNo space available for {filename}. Skipping...\\n\")\n",
        "        else:\n",
        "            print(f\"\\n\\nSkipping {model['filename']} (Disabled)\\n\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Example combined model inputs (both Hugging Face and CivitAI)\n",
        "model_list = [\n",
        "\n",
        "    {\"link\": \"url_link_1\",\n",
        "     \"filename\": \"model1.safetensors\",     \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_2\",\n",
        "     \"filename\": \"model2.safetensors\",    \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_3\",\n",
        "     \"filename\": \"model3.safetensors\",    \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_4\",\n",
        "     \"filename\": \"model4\",    \"enabled\": False  }\n",
        "\n",
        "]\n",
        "\n",
        "# ----------------------------------------\n",
        "# Token Section (define them here and create headers below)\n",
        "huggingface_token = \"your_huggingface_token\"\n",
        "civitai_api_key = \"your_civitai_token\"\n",
        "\n",
        "# Now, after tokens are defined, create the headers\n",
        "huggingface_headers = {\n",
        "    \"Authorization\": f\"Bearer {huggingface_token}\"\n",
        "}\n",
        "civitai_headers = {\n",
        "    \"Authorization\": f\"Bearer {civitai_api_key}\"\n",
        "}\n",
        "\n",
        "# Call the combined download function\n",
        "download_models(model_list, huggingface_headers, civitai_headers)\n",
        "\n",
        "# Clear cache after downloads\n",
        "print(\"\\n\\nClearing cache...\\n\")\n",
        "subprocess.run([\"rm\", \"-rf\", \".cache\"])\n",
        "\n",
        "# Final storage check\n",
        "print(\"\\nAll downloads done!\")\n",
        "print(f\"Available space after downloading: {check_storage_space()}\\n\")\n"
      ],
      "metadata": {
        "id": "nRINZYmd_eE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "TRcIqiHcBdiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Download path\n",
        "download_path = \"/home/studio-lab-user/stable-diffusion-webui/models/VAE/\"\n",
        "\n",
        "# Ensure the download path exists, create it if it doesn't\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Helper function to convert bytes into human-readable format\n",
        "def format_size(size):\n",
        "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
        "        if size < 1024:\n",
        "            return f\"{size:.2f} {unit}\"\n",
        "        size /= 1024\n",
        "\n",
        "# Function to check available storage space\n",
        "def check_storage_space():\n",
        "    result = subprocess.run([\"df\", \"-h\", \"--output=avail,target\"], capture_output=True, text=True)\n",
        "    lines = result.stdout.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        if '/home' in line:\n",
        "            available_space = line.split()[0]  # Get available space value\n",
        "            return available_space\n",
        "    return \"0M\"  # Default value if no storage info available\n",
        "\n",
        "# Function to download model with reduced output frequency\n",
        "def download_model(url, filename, headers):\n",
        "    try:\n",
        "        # Automatically handle the file extension based on the filename provided\n",
        "        response = requests.get(url, headers=headers, stream=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            downloaded_size = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "            with open(filename, 'wb') as file:\n",
        "                chunk_size = 1024 * 1024  # Reduce the chunk size\n",
        "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                    if chunk:\n",
        "                        try:\n",
        "                            file.write(chunk)\n",
        "                            downloaded_size += len(chunk)\n",
        "\n",
        "                            if downloaded_size >= 10 * 1024 * 1024:  # Log progress every 10 MB\n",
        "                                elapsed_time = time.time() - start_time\n",
        "                                speed = (downloaded_size / (1024 * 1024)) / elapsed_time\n",
        "                                print(f\"Downloaded {format_size(downloaded_size)} of {format_size(total_size)} at {speed:.2f} MB/s\", end='\\r')\n",
        "                        except OSError as e:\n",
        "                            if \"No space left on device\" in str(e):\n",
        "                                print(f\"\\n\\nError: No space left on device while downloading {filename}.\")\n",
        "                                return \"no_space\"  # Return a specific message if no space\n",
        "                            raise\n",
        "            print(f\"\\n\\nDownloaded {filename} successfully!\\n\")\n",
        "            return \"success\"\n",
        "        else:\n",
        "            print(f\"\\n\\nFailed to download {filename}. Status code: {response.status_code}, Message: {response.text}\\n\")\n",
        "            return \"error\"\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nError during download of {filename}: {e}\\n\")\n",
        "        if \"data exceeds\" in str(e).lower():\n",
        "            print(\"Error: Data exceeds Jupyter memory limit. Please download outside Jupyter Lab.\\n\")\n",
        "        return \"error\"\n",
        "\n",
        "# Combined function to handle both Hugging Face and CivitAI links\n",
        "def download_models(model_list, huggingface_headers, civitai_headers):\n",
        "    for model in model_list:\n",
        "        if model[\"enabled\"]:\n",
        "            url = model[\"link\"]\n",
        "            filename = os.path.join(download_path, model[\"filename\"])\n",
        "\n",
        "            # Identify if it's Hugging Face or CivitAI by checking the URL structure\n",
        "            if \"huggingface.co\" in url:\n",
        "                headers = huggingface_headers\n",
        "                platform = \"Hugging Face\"\n",
        "            elif \"civitai.com\" in url:\n",
        "                headers = civitai_headers\n",
        "                platform = \"CivitAI\"\n",
        "            else:\n",
        "                print(f\"\\n\\nUnknown platform for {filename}. Skipping...\\n\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nStarting download from {platform} for {filename}...\\n\")\n",
        "\n",
        "            # Check storage before downloading\n",
        "            available_space = check_storage_space()\n",
        "            print(f\"Available space before downloading: {available_space}\\n\")\n",
        "\n",
        "            if available_space.endswith('G') or available_space.endswith('M'):\n",
        "                result = download_model(url, filename, headers)\n",
        "                if result == \"no_space\":\n",
        "                    print(f\"\\n\\nNo space available. Stopping remaining downloads.\\n\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"\\n\\nNo space available for {filename}. Skipping...\\n\")\n",
        "        else:\n",
        "            print(f\"\\n\\nSkipping {model['filename']} (Disabled)\\n\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# Example combined model inputs (both Hugging Face and CivitAI)\n",
        "model_list = [\n",
        "\n",
        "    {\"link\": \"url_link_1\",\n",
        "     \"filename\": \"model1.safetensors\",     \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_2\",\n",
        "     \"filename\": \"model2.safetensors\",    \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_3\",\n",
        "     \"filename\": \"model3.safetensors\",    \"enabled\": False  },\n",
        "\n",
        "    {\"link\": \"url_link_4\",\n",
        "     \"filename\": \"model4\",    \"enabled\": False  }\n",
        "\n",
        "]\n",
        "\n",
        "# ----------------------------------------\n",
        "# Token Section (define them here and create headers below)\n",
        "huggingface_token = \"your_huggingface_token\"\n",
        "civitai_api_key = \"your_civitai_token\"\n",
        "\n",
        "# Now, after tokens are defined, create the headers\n",
        "huggingface_headers = {\n",
        "    \"Authorization\": f\"Bearer {huggingface_token}\"\n",
        "}\n",
        "civitai_headers = {\n",
        "    \"Authorization\": f\"Bearer {civitai_api_key}\"\n",
        "}\n",
        "\n",
        "# Call the combined download function\n",
        "download_models(model_list, huggingface_headers, civitai_headers)\n",
        "\n",
        "# Clear cache after downloads\n",
        "print(\"\\n\\nClearing cache...\\n\")\n",
        "subprocess.run([\"rm\", \"-rf\", \".cache\"])\n",
        "\n",
        "# Final storage check\n",
        "print(\"\\nAll downloads done!\")\n",
        "print(f\"Available space after downloading: {check_storage_space()}\\n\")\n"
      ],
      "metadata": {
        "id": "cZF1TMKJBfvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AvzvbS1hGgft",
        "D3FPHTicN93T",
        "E_aeRowA_bXK",
        "TRcIqiHcBdiJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}